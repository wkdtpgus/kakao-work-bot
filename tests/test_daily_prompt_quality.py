"""
Daily Prompt ê²½ëŸ‰í™” ë²„ì „ í’ˆì§ˆ í…ŒìŠ¤íŠ¸
ê³¨ë“  ë°ì´í„°ì…‹ + LLM-as-a-Judge í‰ê°€ ë°©ì‹
"""
import asyncio
import os
from pathlib import Path
from langchain_core.messages import SystemMessage, HumanMessage
from src.utils.models import get_chat_llm

# GCP ì¸ì¦ ì„¤ì •
project_root = Path(__file__).parent
credentials_path = project_root / "thetimecollabo-38646deba34a.json"
if credentials_path.exists():
    os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = str(credentials_path)
    print(f"âœ… GCP ì¸ì¦ íŒŒì¼ ì„¤ì •: {credentials_path}")
else:
    print(f"âš ï¸ GCP ì¸ì¦ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {credentials_path}")

# ============================================================================
# ê³¨ë“  ë°ì´í„°ì…‹: ì‹¤ì œ ì‚¬ìš©ì ì‹œë‚˜ë¦¬ì˜¤
# ============================================================================

CONVERSATION_TEST_CASES = [
    {
        "name": "ê°„ë‹¨í•œ ì‘ì—… ë³´ê³ ",
        "user_metadata": {"name": "ê¹€ë¯¼ìˆ˜", "job_title": "ë°±ì—”ë“œ ê°œë°œì"},
        "user_message": "ì˜¤ëŠ˜ API ê°œë°œí–ˆì–´",
        "context": {"history": []},
        "expected_behavior": {
            "asks_follow_up": True,
            "asks_about": ["êµ¬ì²´ì  ê¸°ëŠ¥", "ëª©ì ", "ë°©ë²•"],
            "tone": "warm_professional",
            "length": "2-3_sentences"
        }
    },
    {
        "name": "ìƒì„¸í•œ ì‘ì—… ì„¤ëª…",
        "user_metadata": {"name": "ì´ì„œì—°", "job_title": "í”„ë¡ íŠ¸ì—”ë“œ ê°œë°œì"},
        "user_message": "ì‚¬ìš©ì ì¸ì¦ í˜ì´ì§€ë¥¼ ë¦¬ì•¡íŠ¸ë¡œ êµ¬í˜„í–ˆì–´ìš”. JWT í† í° ë°©ì‹ì„ ì‚¬ìš©í–ˆê³ , ë¡œê·¸ì¸ ìƒíƒœë¥¼ ì „ì—­ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ì»¨í…ìŠ¤íŠ¸ë„ ë§Œë“¤ì—ˆì–´ìš”",
        "context": {"history": []},
        "expected_behavior": {
            "asks_follow_up": True,
            "asks_about": ["ê²°ê³¼", "ì–´ë ¤ì›€", "ë‹¤ìŒ ë‹¨ê³„"],
            "acknowledges_detail": True,
            "tone": "warm_professional"
        }
    },
    {
        "name": "ì‚¬ìš©ì ë¶€ì • ì‘ë‹µ",
        "user_metadata": {"name": "ë°•ì¤€í˜¸", "job_title": "ë°ì´í„° ë¶„ì„ê°€"},
        "user_message": "ì•ˆí–ˆì–´",
        "context": {
            "history": [
                {"role": "assistant", "content": "ë°ì´í„° ì‹œê°í™” ì‘ì—…ë„ í•˜ì…¨ë‚˜ìš”?"}
            ]
        },
        "expected_behavior": {
            "acknowledges_denial": True,
            "asks_clarification": True,
            "no_wrong_assumptions": True,
            "tone": "understanding"
        }
    },
    {
        "name": "ì¸ì‚¬/ì¡ë‹´",
        "user_metadata": {"name": "ìµœìœ ì§„", "job_title": "ê¸°íšì"},
        "user_message": "ì•ˆë…•í•˜ì„¸ìš”!",
        "context": {"history": []},
        "expected_behavior": {
            "responds_warmly": True,
            "redirects_to_work": True,
            "mentions": "ì˜¤ëŠ˜ ì—…ë¬´"
        }
    },
    {
        "name": "ì˜¨ë³´ë”© ì¬ì‹œì‘ ìš”ì²­",
        "user_metadata": {"name": "ì •ë¯¼ì§€", "job_title": "ë””ìì´ë„ˆ"},
        "user_message": "ì˜¨ë³´ë”© ë‹¤ì‹œ",
        "context": {"history": []},
        "expected_behavior": {
            "prevents_restart": True,
            "mentions": "ì´ë¯¸ ì™„ë£Œ",
            "redirects_to_work": True
        }
    }
]

SUMMARY_TEST_CASES = [
    {
        "name": "ê°„ë‹¨í•œ ìš”ì•½",
        "user_metadata": {"job_title": "ê°œë°œì"},
        "conversation": "ì˜¤ëŠ˜ API 3ê°œ ê°œë°œí–ˆì–´ìš”. ì‚¬ìš©ì ì¸ì¦, ë°ì´í„° ì¡°íšŒ, íŒŒì¼ ì—…ë¡œë“œ APIì˜ˆìš”.",
        "expected_quality": {
            "includes_numbers": True,
            "under_900_chars": True,
            "has_encouragement": True,
            "has_suggestions": 2,
            "no_markdown": True
        }
    },
    {
        "name": "ë¶€ì • ë‚´ìš© í¬í•¨",
        "user_metadata": {"job_title": "ê¸°íšì"},
        "conversation": "ì˜¤ëŠ˜ ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œ ì‘ì„±í–ˆì–´ìš”. ì™€ì´ì–´í”„ë ˆì„ë„ ê·¸ë¦¬ë ¤ê³  í–ˆëŠ”ë° ê·¸ê±´ ì•ˆí–ˆì–´ìš”. ì‹œê°„ì´ ë¶€ì¡±í–ˆê±°ë“ ìš”.",
        "expected_quality": {
            "excludes_denied_content": True,  # ì™€ì´ì–´í”„ë ˆì„ ì œì™¸
            "includes_only_completed": True,  # ìš”êµ¬ì‚¬í•­ ì •ì˜ì„œë§Œ
            "under_900_chars": True,
            "has_suggestions": 2
        }
    },
    {
        "name": "ìˆ˜ì¹˜ í¬í•¨ ìƒì„¸ ì‘ì—…",
        "user_metadata": {"job_title": "ë°ì´í„° ë¶„ì„ê°€"},
        "conversation": "500ê±´ì˜ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ë¶„ì„í•´ì„œ 3ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜í–ˆì–´ìš”. ê° ìœ í˜•ë³„ë¡œ íŠ¹ì§•ì„ ì •ë¦¬í•˜ê³  ì‹œê°í™” ì°¨íŠ¸ë„ ë§Œë“¤ì—ˆì–´ìš”.",
        "expected_quality": {
            "includes_numbers": True,  # 500ê±´, 3ê°€ì§€
            "specific_achievements": True,
            "under_900_chars": True,
            "actionable_suggestions": True
        }
    }
]

# ============================================================================
# LLM-as-a-Judge í‰ê°€ê¸°
# ============================================================================

CONVERSATION_EVALUATION_PROMPT = """
You are an expert evaluator of AI chatbot responses.

# Task
Evaluate the chatbot's response to a user message based on the criteria below.

# User Context
- Message: "{user_message}"
- Expected Behavior: {expected_behavior}

# Chatbot Response
{bot_response}

# Evaluation Criteria (rate 1-5 for each)
1. **Follow-up Quality**: Does it ask relevant, thoughtful follow-up questions?
2. **Tone Appropriateness**: Is the tone warm, professional, and appropriate?
3. **Length**: Is it concise (2-3 sentences)?
4. **Context Awareness**: Does it properly handle denials, greetings, or special requests?
5. **Korean Quality**: Is the Korean natural and grammatically correct?

# Output Format (JSON)
{{
  "follow_up_quality": <1-5>,
  "tone": <1-5>,
  "length": <1-5>,
  "context_awareness": <1-5>,
  "korean_quality": <1-5>,
  "overall_score": <average>,
  "reasoning": "Brief explanation of scores"
}}
"""

SUMMARY_EVALUATION_PROMPT = """
You are an expert evaluator of career summaries.

# Task
Evaluate the quality of a career memo based on the criteria below.

# Original Conversation
{conversation}

# Generated Summary
{summary}

# Expected Quality
{expected_quality}

# Evaluation Criteria (rate 1-5 for each)
1. **Factual Accuracy**: Does it only include what the user confirmed? Excludes denied content?
2. **Conciseness**: Is it under 900 Korean characters?
3. **Specificity**: Does it include specific numbers, methods, and outcomes?
4. **Format Compliance**: Emojis (ğŸ“, ğŸ’¡) are ALLOWED. Check: no bold (**text**), no headers (#), no bullets (*/-). Correct structure?
5. **Actionability**: Are the next-day suggestions concrete and useful?
6. **Korean Quality**: Natural, professional Korean with ~í•¨ style?

# Output Format (JSON)
{{
  "factual_accuracy": <1-5>,
  "conciseness": <1-5>,
  "specificity": <1-5>,
  "format_compliance": <1-5>,
  "actionability": <1-5>,
  "korean_quality": <1-5>,
  "overall_score": <average>,
  "character_count": <actual count>,
  "reasoning": "Brief explanation of scores"
}}
"""

# ============================================================================
# Pairwise Comparison í‰ê°€ (ê°œì„  ë°©ë²• #1)
# ============================================================================

PAIRWISE_CONVERSATION_PROMPT = """
You are an expert evaluator comparing two AI chatbot responses.

# Task
Compare Response A and Response B, then choose which is better.

# User Context
- Message: "{user_message}"
- Expected Behavior: {expected_behavior}

# Response A
{response_a}

# Response B
{response_b}

# Comparison Criteria
1. **Follow-up Quality**: Which asks more thoughtful, relevant questions?
2. **Tone**: Which has a more appropriate warm/professional tone?
3. **Length**: Which is more concise (2-3 sentences)?
4. **Context Awareness**: Which better handles the specific situation (denial/greeting/etc)?
5. **Korean Quality**: Which has more natural, grammatical Korean?

# Output Format (JSON)
{{
  "winner": "A" | "B" | "Tie",
  "confidence": "high" | "medium" | "low",
  "better_at": {{
    "follow_up_quality": "A" | "B" | "Tie",
    "tone": "A" | "B" | "Tie",
    "length": "A" | "B" | "Tie",
    "context_awareness": "A" | "B" | "Tie",
    "korean_quality": "A" | "B" | "Tie"
  }},
  "reasoning": "Brief explanation of why the winner is better"
}}
"""

PAIRWISE_SUMMARY_PROMPT = """
You are an expert evaluator comparing two career summaries.

# Task
Compare Summary A and Summary B, then choose which is better.

# Original Conversation
{conversation}

# Summary A
{summary_a}

# Summary B
{summary_b}

# Expected Quality
{expected_quality}

# Comparison Criteria
1. **Factual Accuracy**: Which better includes only confirmed content and excludes denials?
2. **Conciseness**: Which is more concise while staying under 900 chars?
3. **Specificity**: Which includes more specific numbers, methods, outcomes?
4. **Format Compliance**: Emojis (ğŸ“, ğŸ’¡) are ALLOWED. Which better avoids forbidden Markdown (bold/headers/bullets) and follows structure?
5. **Actionability**: Which provides more concrete, useful next-day suggestions?
6. **Korean Quality**: Which has more natural, professional Korean?

# Output Format (JSON)
{{
  "winner": "A" | "B" | "Tie",
  "confidence": "high" | "medium" | "low",
  "better_at": {{
    "factual_accuracy": "A" | "B" | "Tie",
    "conciseness": "A" | "B" | "Tie",
    "specificity": "A" | "B" | "Tie",
    "format_compliance": "A" | "B" | "Tie",
    "actionability": "A" | "B" | "Tie",
    "korean_quality": "A" | "B" | "Tie"
  }},
  "reasoning": "Brief explanation of why the winner is better",
  "char_count_a": <count>,
  "char_count_b": <count>
}}
"""

# ============================================================================
# Chain-of-Thought í‰ê°€ (ê°œì„  ë°©ë²• #3)
# ============================================================================

COT_CONVERSATION_PROMPT = """
You are an expert evaluator of AI chatbot responses. Use step-by-step reasoning.

# Task
Evaluate the chatbot's response using Chain-of-Thought reasoning.

# User Context
- Message: "{user_message}"
- Expected Behavior: {expected_behavior}

# Chatbot Response
{bot_response}

# Evaluation Process (Think step by step)

## Step 1: Analyze the user's situation
- What is the user trying to do?
- What does the expected behavior suggest?
- What would be an ideal response?

## Step 2: Evaluate each criterion (1-5 scale)
1. **Follow-up Quality**: Does it ask relevant, thoughtful questions?
   - Think: What questions would help the user reflect on their work?

2. **Tone Appropriateness**: Is it warm, professional, appropriate?
   - Think: Does this match the expected tone for a career mentor?

3. **Length**: Is it concise (2-3 sentences)?
   - Think: Count the sentences. Is it too verbose or too short?

4. **Context Awareness**: Does it handle denials/greetings/special requests properly?
   - Think: Does this response show understanding of the specific situation?

5. **Korean Quality**: Is the Korean natural and grammatically correct?
   - Think: Are there any awkward phrases or grammar errors?

## Step 3: Final judgment
- Calculate overall score
- Identify strengths and weaknesses
- Provide actionable feedback

# Output Format (JSON)
{{
  "step1_analysis": "Your analysis of the user's situation",
  "step2_reasoning": {{
    "follow_up_quality": {{"score": <1-5>, "reasoning": "..."}},
    "tone": {{"score": <1-5>, "reasoning": "..."}},
    "length": {{"score": <1-5>, "reasoning": "..."}},
    "context_awareness": {{"score": <1-5>, "reasoning": "..."}},
    "korean_quality": {{"score": <1-5>, "reasoning": "..."}}
  }},
  "step3_final": {{
    "overall_score": <average>,
    "strengths": ["strength1", "strength2"],
    "weaknesses": ["weakness1", "weakness2"],
    "actionable_feedback": "Specific suggestions for improvement"
  }}
}}
"""

COT_SUMMARY_PROMPT = """
You are an expert evaluator of career summaries. Use step-by-step reasoning.

# Task
Evaluate the career memo using Chain-of-Thought reasoning.

# Original Conversation
{conversation}

# Generated Summary
{summary}

# Expected Quality
{expected_quality}

# Evaluation Process (Think step by step)

## Step 1: Analyze the conversation
- What tasks did the user actually complete?
- What did they deny or say they didn't do?
- What specific details (numbers, methods) were mentioned?

## Step 2: Evaluate each criterion (1-5 scale)
1. **Factual Accuracy**: Only confirmed content, excludes denials?
   - Think: Cross-check each claim in summary against conversation

2. **Conciseness**: Under 900 Korean characters?
   - Think: Count characters. Is it concise yet complete?

3. **Specificity**: Includes specific numbers, methods, outcomes?
   - Think: Are details from conversation preserved?

4. **Format Compliance**: Emojis (ğŸ“, ğŸ’¡) are ALLOWED. Check: no bold (**text**), no headers (#), no bullets (*/-). Correct structure?
   - Think: Are there any forbidden Markdown elements? (Emojis are OK)

5. **Actionability**: Concrete, useful next-day suggestions?
   - Think: Are suggestions specific enough to act on?

6. **Korean Quality**: Natural, professional, ~í•¨ style?
   - Think: Is the Korean grammatical and appropriate?

## Step 3: Final judgment
- Calculate overall score
- Identify what's done well and what needs improvement
- Provide specific feedback

# Output Format (JSON)
{{
  "step1_analysis": {{
    "completed_tasks": ["task1", "task2"],
    "denied_tasks": ["task1"],
    "key_details": ["detail1", "detail2"]
  }},
  "step2_reasoning": {{
    "factual_accuracy": {{"score": <1-5>, "reasoning": "..."}},
    "conciseness": {{"score": <1-5>, "reasoning": "...", "char_count": <count>}},
    "specificity": {{"score": <1-5>, "reasoning": "..."}},
    "format_compliance": {{"score": <1-5>, "reasoning": "..."}},
    "actionability": {{"score": <1-5>, "reasoning": "..."}},
    "korean_quality": {{"score": <1-5>, "reasoning": "..."}}
  }},
  "step3_final": {{
    "overall_score": <average>,
    "strengths": ["strength1", "strength2"],
    "weaknesses": ["weakness1", "weakness2"],
    "actionable_feedback": "Specific suggestions for improvement"
  }}
}}
"""

# ============================================================================
# í…ŒìŠ¤íŠ¸ ì‹¤í–‰ í•¨ìˆ˜ë“¤
# ============================================================================

async def test_conversation_prompt(system_prompt: str, test_case: dict, prompt_name: str):
    """ëŒ€í™” í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸"""
    llm = get_chat_llm()

    # í”„ë¡¬í”„íŠ¸ ë Œë”ë§
    rendered_prompt = system_prompt.format(
        name=test_case["user_metadata"]["name"],
        job_title=test_case["user_metadata"]["job_title"],
        total_years="3ë…„",  # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°
        job_years="1ë…„",
        career_goal="ì‹œë‹ˆì–´ ê°œë°œì",
        project_name="í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸",
        recent_work="ì±—ë´‡ ê°œë°œ"
    )

    # LLM ì‘ë‹µ ìƒì„±
    messages = [SystemMessage(content=rendered_prompt)]
    if test_case["context"].get("history"):
        for msg in test_case["context"]["history"]:
            messages.append(HumanMessage(content=msg["content"]))
    messages.append(HumanMessage(content=test_case["user_message"]))

    response = await llm.ainvoke(messages)
    bot_response = response.content

    # LLM Judge í‰ê°€
    eval_prompt = CONVERSATION_EVALUATION_PROMPT.format(
        user_message=test_case["user_message"],
        expected_behavior=test_case["expected_behavior"],
        bot_response=bot_response
    )

    eval_response = await llm.ainvoke([HumanMessage(content=eval_prompt)])

    # JSON íŒŒì‹± ì‹œë„
    import json
    try:
        eval_json = json.loads(eval_response.content.replace("```json", "").replace("```", "").strip())
    except:
        eval_json = {"raw": eval_response.content}

    return {
        "test_name": test_case["name"],
        "prompt_version": prompt_name,
        "bot_response": bot_response,
        "evaluation": eval_json,
        "response_length": len(bot_response)
    }


async def test_summary_prompt(system_prompt: str, test_case: dict, prompt_name: str):
    """ìš”ì•½ í”„ë¡¬í”„íŠ¸ í…ŒìŠ¤íŠ¸"""
    llm = get_chat_llm()

    # ìš”ì•½ ìƒì„±
    user_prompt = f"""
# USER_INFO
{test_case["user_metadata"]}

# CONVERSATION
{test_case["conversation"]}
"""

    response = await llm.ainvoke([
        SystemMessage(content=system_prompt),
        HumanMessage(content=user_prompt)
    ])

    summary = response.content
    char_count = len(summary)

    # LLM Judge í‰ê°€
    eval_prompt = SUMMARY_EVALUATION_PROMPT.format(
        conversation=test_case["conversation"],
        summary=summary,
        expected_quality=test_case["expected_quality"]
    )

    eval_response = await llm.ainvoke([HumanMessage(content=eval_prompt)])

    # JSON íŒŒì‹± ì‹œë„
    import json
    try:
        eval_json = json.loads(eval_response.content.replace("```json", "").replace("```", "").strip())
    except:
        eval_json = {"raw": eval_response.content}

    return {
        "test_name": test_case["name"],
        "prompt_version": prompt_name,
        "summary": summary,
        "character_count": char_count,
        "evaluation": eval_json
    }


async def pairwise_comparison_test(prompt_a: str, prompt_b: str, test_case: dict, prompt_name_a: str, prompt_name_b: str):
    """Pairwise Comparison: ë‘ í”„ë¡¬í”„íŠ¸ì˜ ì‘ë‹µì„ ì§ì ‘ ë¹„êµ"""
    llm = get_chat_llm()

    # í”„ë¡¬í”„íŠ¸ A ì‘ë‹µ ìƒì„±
    rendered_a = prompt_a.format(
        name=test_case["user_metadata"]["name"],
        job_title=test_case["user_metadata"]["job_title"],
        total_years="3ë…„", job_years="1ë…„",
        career_goal="ì‹œë‹ˆì–´ ê°œë°œì",
        project_name="í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸",
        recent_work="ì±—ë´‡ ê°œë°œ"
    )
    messages_a = [SystemMessage(content=rendered_a), HumanMessage(content=test_case["user_message"])]
    response_a = await llm.ainvoke(messages_a)

    # í”„ë¡¬í”„íŠ¸ B ì‘ë‹µ ìƒì„±
    rendered_b = prompt_b.format(
        name=test_case["user_metadata"]["name"],
        job_title=test_case["user_metadata"]["job_title"],
        total_years="3ë…„", job_years="1ë…„",
        career_goal="ì‹œë‹ˆì–´ ê°œë°œì",
        project_name="í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸",
        recent_work="ì±—ë´‡ ê°œë°œ"
    )
    messages_b = [SystemMessage(content=rendered_b), HumanMessage(content=test_case["user_message"])]
    response_b = await llm.ainvoke(messages_b)

    # Pairwise í‰ê°€
    eval_prompt = PAIRWISE_CONVERSATION_PROMPT.format(
        user_message=test_case["user_message"],
        expected_behavior=test_case["expected_behavior"],
        response_a=response_a.content,
        response_b=response_b.content
    )
    eval_response = await llm.ainvoke([HumanMessage(content=eval_prompt)])

    import json
    try:
        eval_json = json.loads(eval_response.content.replace("```json", "").replace("```", "").strip())
    except:
        eval_json = {"raw": eval_response.content}

    return {
        "test_name": test_case["name"],
        "prompt_a_name": prompt_name_a,
        "prompt_b_name": prompt_name_b,
        "response_a": response_a.content,
        "response_b": response_b.content,
        "comparison": eval_json
    }


async def cot_evaluation_test(system_prompt: str, test_case: dict, is_summary: bool = False):
    """Chain-of-Thought Evaluation: ë‹¨ê³„ë³„ ì¶”ë¡ ì„ í¬í•¨í•œ í‰ê°€"""
    llm = get_chat_llm()

    if is_summary:
        # ìš”ì•½ ìƒì„±
        user_prompt = f"""
# USER_INFO
{test_case["user_metadata"]}

# CONVERSATION
{test_case["conversation"]}
"""
        response = await llm.ainvoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ])

        # CoT í‰ê°€
        eval_prompt = COT_SUMMARY_PROMPT.format(
            conversation=test_case["conversation"],
            summary=response.content,
            expected_quality=test_case["expected_quality"]
        )
    else:
        # ëŒ€í™” ì‘ë‹µ ìƒì„±
        rendered = system_prompt.format(
            name=test_case["user_metadata"]["name"],
            job_title=test_case["user_metadata"]["job_title"],
            total_years="3ë…„", job_years="1ë…„",
            career_goal="ì‹œë‹ˆì–´ ê°œë°œì",
            project_name="í…ŒìŠ¤íŠ¸ í”„ë¡œì íŠ¸",
            recent_work="ì±—ë´‡ ê°œë°œ"
        )
        messages = [SystemMessage(content=rendered), HumanMessage(content=test_case["user_message"])]
        response = await llm.ainvoke(messages)

        # CoT í‰ê°€
        eval_prompt = COT_CONVERSATION_PROMPT.format(
            user_message=test_case["user_message"],
            expected_behavior=test_case["expected_behavior"],
            bot_response=response.content
        )

    eval_response = await llm.ainvoke([HumanMessage(content=eval_prompt)])

    import json
    try:
        eval_json = json.loads(eval_response.content.replace("```json", "").replace("```", "").strip())
    except:
        eval_json = {"raw": eval_response.content}

    return {
        "test_name": test_case["name"],
        "bot_response": response.content,
        "cot_evaluation": eval_json
    }


async def main():
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""

    # í”„ë¡¬í”„íŠ¸ import
    from src.prompt.daily_record_prompt import DAILY_CONVERSATION_SYSTEM_PROMPT as CONV_PROMPT
    from src.prompt.daily_summary_prompt import DAILY_SUMMARY_SYSTEM_PROMPT as SUM_PROMPT

    print("\n" + "="*70)
    print("ğŸ“‹ Daily Conversation Prompt í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
    print("="*70)

    conv_results = []
    for test_case in CONVERSATION_TEST_CASES:
        print(f"\ní…ŒìŠ¤íŠ¸: {test_case['name']}")
        result = await test_conversation_prompt(CONV_PROMPT, test_case, "ê²½ëŸ‰í™” ë²„ì „")
        conv_results.append(result)

        print(f"  ğŸ“¨ ì‚¬ìš©ì: {test_case['user_message']}")
        print(f"  ğŸ¤– ë´‡ ì‘ë‹µ: {result['bot_response']}")
        print(f"  ğŸ“ ì‘ë‹µ ê¸¸ì´: {result['response_length']}ì")

        if isinstance(result['evaluation'], dict):
            if 'overall_score' in result['evaluation']:
                print(f"  â­ ì¢…í•© ì ìˆ˜: {result['evaluation']['overall_score']}/5")
                print(f"     - Follow-up ì§ˆë¬¸: {result['evaluation'].get('follow_up_quality', 'N/A')}/5")
                print(f"     - í†¤: {result['evaluation'].get('tone', 'N/A')}/5")
                print(f"     - ê¸¸ì´: {result['evaluation'].get('length', 'N/A')}/5")
                print(f"     - ë§¥ë½ ì¸ì‹: {result['evaluation'].get('context_awareness', 'N/A')}/5")
                print(f"     - í•œêµ­ì–´ í’ˆì§ˆ: {result['evaluation'].get('korean_quality', 'N/A')}/5")
                if 'reasoning' in result['evaluation']:
                    print(f"  ğŸ’­ í‰ê°€ ì´ìœ : {result['evaluation']['reasoning']}")
        else:
            print(f"  âš ï¸ í‰ê°€ (íŒŒì‹± ì‹¤íŒ¨): {str(result['evaluation'])[:200]}...")

    print("\n" + "="*70)
    print("ğŸ“ Daily Summary Prompt í’ˆì§ˆ í…ŒìŠ¤íŠ¸")
    print("="*70)

    sum_results = []
    for test_case in SUMMARY_TEST_CASES:
        print(f"\ní…ŒìŠ¤íŠ¸: {test_case['name']}")
        result = await test_summary_prompt(SUM_PROMPT, test_case, "ê²½ëŸ‰í™” ë²„ì „")
        sum_results.append(result)

        print(f"  ğŸ’¬ ëŒ€í™”: {test_case['conversation']}")
        print(f"  ğŸ“ ìš”ì•½:")
        print("  " + "\n  ".join(result['summary'].split('\n')))
        print(f"  ğŸ“ ê¸€ì ìˆ˜: {result['character_count']}/900")

        if isinstance(result['evaluation'], dict):
            if 'overall_score' in result['evaluation']:
                print(f"  â­ ì¢…í•© ì ìˆ˜: {result['evaluation']['overall_score']}/5")
                print(f"     - ì‚¬ì‹¤ ì •í™•ì„±: {result['evaluation'].get('factual_accuracy', 'N/A')}/5")
                print(f"     - ê°„ê²°ì„±: {result['evaluation'].get('conciseness', 'N/A')}/5")
                print(f"     - êµ¬ì²´ì„±: {result['evaluation'].get('specificity', 'N/A')}/5")
                print(f"     - í˜•ì‹ ì¤€ìˆ˜: {result['evaluation'].get('format_compliance', 'N/A')}/5")
                print(f"     - ì‹¤í–‰ê°€ëŠ¥ì„±: {result['evaluation'].get('actionability', 'N/A')}/5")
                print(f"     - í•œêµ­ì–´ í’ˆì§ˆ: {result['evaluation'].get('korean_quality', 'N/A')}/5")
                if 'reasoning' in result['evaluation']:
                    print(f"  ğŸ’­ í‰ê°€ ì´ìœ : {result['evaluation']['reasoning']}")
        else:
            print(f"  âš ï¸ í‰ê°€ (íŒŒì‹± ì‹¤íŒ¨): {str(result['evaluation'])[:200]}...")

    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print("\n" + "="*70)
    print("ğŸ“Š ì¢…í•© í‰ê°€ ê²°ê³¼")
    print("="*70)

    # ëŒ€í™” í”„ë¡¬í”„íŠ¸ í†µê³„
    print(f"\n[ëŒ€í™” í”„ë¡¬í”„íŠ¸]")
    print(f"  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(conv_results)}ê°œ")
    print(f"  í‰ê·  ì‘ë‹µ ê¸¸ì´: {sum(r['response_length'] for r in conv_results) / len(conv_results):.0f}ì")

    conv_scores = [r['evaluation'].get('overall_score', 0) for r in conv_results if isinstance(r['evaluation'], dict) and 'overall_score' in r['evaluation']]
    if conv_scores:
        print(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {sum(conv_scores) / len(conv_scores):.2f}/5")
        print(f"  ìµœê³  ì ìˆ˜: {max(conv_scores)}/5")
        print(f"  ìµœì € ì ìˆ˜: {min(conv_scores)}/5")

    # ìš”ì•½ í”„ë¡¬í”„íŠ¸ í†µê³„
    print(f"\n[ìš”ì•½ í”„ë¡¬í”„íŠ¸]")
    print(f"  í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(sum_results)}ê°œ")
    print(f"  í‰ê·  ê¸€ì ìˆ˜: {sum(r['character_count'] for r in sum_results) / len(sum_results):.0f}/900")

    over_limit = [r for r in sum_results if r['character_count'] > 900]
    if over_limit:
        print(f"  âš ï¸ 900ì ì´ˆê³¼: {len(over_limit)}ê±´")
        for r in over_limit:
            print(f"     - {r['test_name']}: {r['character_count']}ì")
    else:
        print(f"  âœ… ëª¨ë“  ìš”ì•½ì´ 900ì ì´ë‚´")

    sum_scores = [r['evaluation'].get('overall_score', 0) for r in sum_results if isinstance(r['evaluation'], dict) and 'overall_score' in r['evaluation']]
    if sum_scores:
        print(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {sum(sum_scores) / len(sum_scores):.2f}/5")
        print(f"  ìµœê³  ì ìˆ˜: {max(sum_scores)}/5")
        print(f"  ìµœì € ì ìˆ˜: {min(sum_scores)}/5")

    # ê°œì„  í¬ì¸íŠ¸ ë¶„ì„
    print(f"\n[ê°œì„ ì´ í•„ìš”í•œ ì˜ì—­]")
    for result in conv_results:
        if isinstance(result['evaluation'], dict) and 'overall_score' in result['evaluation']:
            if result['evaluation']['overall_score'] < 4.0:
                print(f"  âš ï¸ ëŒ€í™”: {result['test_name']} (ì ìˆ˜: {result['evaluation']['overall_score']}/5)")

    for result in sum_results:
        if isinstance(result['evaluation'], dict) and 'overall_score' in result['evaluation']:
            if result['evaluation']['overall_score'] < 4.0:
                print(f"  âš ï¸ ìš”ì•½: {result['test_name']} (ì ìˆ˜: {result['evaluation']['overall_score']}/5)")

    print("\n" + "="*70)
    print("âœ… í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("="*70)
    print("\nğŸ’¡ ë‹¤ìŒ ë‹¨ê³„:")
    print("  1. í‰ê°€ ê²°ê³¼ë¥¼ ê²€í† í•˜ì—¬ ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ íŒŒì•…")
    print("  2. í‰ê·  ì ìˆ˜ 4.0 ë¯¸ë§Œì¸ ì¼€ì´ìŠ¤ ë¶„ì„ ë° í”„ë¡¬í”„íŠ¸ ë¯¸ì„¸ ì¡°ì •")
    print("  3. ì‹¤ì œ í™˜ê²½ì— ë°°í¬ ì „ A/B í…ŒìŠ¤íŠ¸ ê³ ë ¤")
    print("  4. ì‘ë‹µ ì‹œê°„ ì¸¡ì •í•˜ì—¬ ê²½ëŸ‰í™” íš¨ê³¼ í™•ì¸")

    # ========================================================================
    # ì¶”ê°€ í‰ê°€ ë°©ë²• ì‹œì—° (Pairwise Comparison & Chain-of-Thought)
    # ========================================================================
    print("\n" + "="*70)
    print("ğŸ†š Pairwise Comparison í…ŒìŠ¤íŠ¸ (ê²½ëŸ‰í™” ì „/í›„ ì§ì ‘ ë¹„êµ)")
    print("="*70)
    print("\nğŸ’¡ ì‚¬ìš©ë²•: ë‘ í”„ë¡¬í”„íŠ¸ ë²„ì „ì„ ì§ì ‘ ë¹„êµí•˜ë ¤ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”")
    print("""
# ì˜ˆì‹œ: ê²½ëŸ‰í™” ì „ í”„ë¡¬í”„íŠ¸ì™€ í˜„ì¬ í”„ë¡¬í”„íŠ¸ ë¹„êµ
# ORIGINAL_PROMPT = '''ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ ë‚´ìš©...'''
#
# for test_case in CONVERSATION_TEST_CASES[:2]:  # ìƒ˜í”Œ 2ê°œë§Œ
#     result = await pairwise_comparison_test(
#         prompt_a=ORIGINAL_PROMPT,
#         prompt_b=CONV_PROMPT,
#         test_case=test_case,
#         prompt_name_a="ê²½ëŸ‰í™” ì „",
#         prompt_name_b="ê²½ëŸ‰í™” í›„"
#     )
#     print(f"\\ní…ŒìŠ¤íŠ¸: {result['test_name']}")
#     print(f"  ìŠ¹ì: {result['comparison'].get('winner', 'N/A')}")
#     print(f"  ì‹ ë¢°ë„: {result['comparison'].get('confidence', 'N/A')}")
#     print(f"  ì´ìœ : {result['comparison'].get('reasoning', 'N/A')}")
""")

    print("\n" + "="*70)
    print("ğŸ§  Chain-of-Thought Evaluation í…ŒìŠ¤íŠ¸ (ë‹¨ê³„ë³„ ì¶”ë¡  í‰ê°€)")
    print("="*70)
    print("\nğŸ’¡ ì‚¬ìš©ë²•: ë” ìƒì„¸í•œ í‰ê°€ê°€ í•„ìš”í•˜ë©´ ì•„ë˜ ì£¼ì„ì„ í•´ì œí•˜ì„¸ìš”")
    print("""
# ì˜ˆì‹œ: CoT í‰ê°€ë¡œ ë” ê¹Šì€ ì¸ì‚¬ì´íŠ¸ ì–»ê¸°
# for test_case in CONVERSATION_TEST_CASES[:1]:  # ìƒ˜í”Œ 1ê°œë§Œ
#     result = await cot_evaluation_test(CONV_PROMPT, test_case, is_summary=False)
#     cot = result['cot_evaluation']
#
#     print(f"\\ní…ŒìŠ¤íŠ¸: {result['test_name']}")
#     if 'step1_analysis' in cot:
#         print(f"  [Step 1] ìƒí™© ë¶„ì„: {cot['step1_analysis']}")
#     if 'step2_reasoning' in cot:
#         print(f"  [Step 2] ì„¸ë¶€ í‰ê°€:")
#         for criterion, data in cot['step2_reasoning'].items():
#             print(f"    - {criterion}: {data.get('score', 'N/A')}/5")
#             print(f"      ì´ìœ : {data.get('reasoning', 'N/A')}")
#     if 'step3_final' in cot:
#         final = cot['step3_final']
#         print(f"  [Step 3] ìµœì¢… í‰ê°€:")
#         print(f"    - ì¢…í•© ì ìˆ˜: {final.get('overall_score', 'N/A')}/5")
#         print(f"    - ê°•ì : {', '.join(final.get('strengths', []))}")
#         print(f"    - ì•½ì : {', '.join(final.get('weaknesses', []))}")
#         print(f"    - ê°œì„  ì œì•ˆ: {final.get('actionable_feedback', 'N/A')}")
""")


if __name__ == "__main__":
    asyncio.run(main())

"""
LangGraph ì˜¨ë³´ë”© ì›Œí¬í”Œë¡œìš°
"""

from typing import Dict, Any
from functools import partial
import os
from langgraph.graph import StateGraph, END
from .state import OnboardingState, OnboardingResponse
from .utils import ResponseFormatter, PromptLoader, get_openai_model
from .memory_manager import MemoryManager
from . import nodes


def route_next_step(state: OnboardingState) -> str:
    """ë‹¤ìŒ ë‹¨ê³„ ë¼ìš°íŒ…"""
    return state.get("next_step", "continue_onboarding")


def build_workflow_graph(db, memory_manager, llm, prompt_loader) -> StateGraph:
    """ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ êµ¬ì„±"""

    # StateGraph ìƒì„±
    workflow = StateGraph(OnboardingState)

    # ê¸°ë³¸ ë…¸ë“œë“¤ ì¶”ê°€ (partialë¡œ ì˜ì¡´ì„± ì£¼ì…)
    workflow.add_node("load_user_state",
                     partial(nodes.load_user_state, db=db, memory_manager=memory_manager))
    workflow.add_node("check_next_step",
                     partial(nodes.check_next_step, db=db))

    # ì˜¨ë³´ë”© ë…¸ë“œë“¤
    workflow.add_node("generate_ai_response",
                     partial(nodes.generate_ai_response, llm=llm, prompt_loader=prompt_loader))
    workflow.add_node("update_user_info",
                     partial(nodes.update_user_info, db=db, memory_manager=memory_manager))

    # ì¼ì¼ íšŒê³  ë…¸ë“œë“¤
    workflow.add_node("start_daily_reflection", nodes.start_daily_reflection)
    workflow.add_node("collect_daily_tasks", nodes.collect_daily_tasks)

    # ì£¼ê°„ ë©ì—… ë…¸ë“œë“¤
    workflow.add_node("start_weekly_wrapup", nodes.start_weekly_wrapup)
    workflow.add_node("generate_weekly_insights", nodes.generate_weekly_insights)
    workflow.add_node("save_weekly_summary", nodes.save_weekly_summary)

    # ê³µí†µ ì €ì¥ ë…¸ë“œ
    workflow.add_node("save_conversation",
                     partial(nodes.save_conversation, memory_manager=memory_manager, db=db))

    # ì—£ì§€ ì •ì˜
    workflow.set_entry_point("load_user_state")

    # ê¸°ë³¸ í”Œë¡œìš°
    workflow.add_edge("load_user_state", "check_next_step")

    # ì¡°ê±´ë¶€ ë¼ìš°íŒ…
    workflow.add_conditional_edges(
        "check_next_step",
        route_next_step,
        {
            "continue_onboarding": "generate_ai_response",
            "daily_reflection": "start_daily_reflection",
            "weekly_wrapup": "start_weekly_wrapup"
        }
    )

    # ì˜¨ë³´ë”© í”Œë¡œìš°
    workflow.add_edge("generate_ai_response", "update_user_info")
    workflow.add_edge("update_user_info", "save_conversation")

    # ì¼ì¼ íšŒê³  í”Œë¡œìš°
    workflow.add_edge("start_daily_reflection", "save_conversation")
    # TODO: ë‚˜ì¤‘ì— ì‹¤ì œ ëŒ€í™” í”Œë¡œìš° êµ¬í˜„ì‹œ ë” ë³µì¡í•œ ì—°ê²°

    # ì£¼ê°„ ë©ì—… í”Œë¡œìš°
    workflow.add_edge("start_weekly_wrapup", "generate_weekly_insights")
    workflow.add_edge("generate_weekly_insights", "save_weekly_summary")
    workflow.add_edge("save_weekly_summary", "save_conversation")

    # ìµœì¢… ì¢…ë£Œ
    workflow.add_edge("save_conversation", END)

    return workflow.compile()


# ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € (ì‚¬ìš©ìë³„ ìºì‹œ ìœ ì§€ìš©)
_global_memory_manager = None

async def handle_onboarding_conversation(user_id: str, message: str, db) -> Dict[str, Any]:
    """ì˜¨ë³´ë”© ëŒ€í™” ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    try:
        print(f"ğŸ¤– ì˜¨ë³´ë”© ëŒ€í™” ì‹œì‘: {user_id}")
        print(f"ğŸ“¨ ë°›ì€ ë©”ì‹œì§€: {message}")

        # ê¸€ë¡œë²Œ ë©”ëª¨ë¦¬ ë§¤ë‹ˆì € ì‚¬ìš© (ìºì‹œ ìœ ì§€)
        global _global_memory_manager
        if _global_memory_manager is None:
            _global_memory_manager = MemoryManager()

        memory_manager = _global_memory_manager
        prompt_loader = PromptLoader()
        formatter = ResponseFormatter()
        llm = get_openai_model().with_structured_output(OnboardingResponse)

        # ì›Œí¬í”Œë¡œìš° ê·¸ë˜í”„ ìƒì„±
        graph = build_workflow_graph(db, memory_manager, llm, prompt_loader)

        # ì´ˆê¸° ìƒíƒœ êµ¬ì„±
        initial_state = OnboardingState(
            user_id=user_id,
            message=message,
            current_state={},
            ai_response="",
            updated_variables={},
            conversation_history=[],
            next_step=""
        )

        # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
        final_state = await graph.ainvoke(initial_state)

        # ìµœì¢… ì‘ë‹µ ë°˜í™˜
        ai_response = final_state["ai_response"]
        return formatter.simple_text_response(ai_response)

    except Exception as error:
        print(f"ì˜¨ë³´ë”© ëŒ€í™” ì²˜ë¦¬ ì˜¤ë¥˜: {error}")
        import traceback
        traceback.print_exc()
        formatter = ResponseFormatter()
        return formatter.error_response(
            "AI ëŒ€í™” ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        )


